---
title: "CFB Game Predictive Model"
author: "The Five Horsemen"
date: "2024-10-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Install Packages
```{r, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(rsample)
library(mice)
library(fastDummies)
library(xgboost)
library(Metrics)
library(caret)
```

## Load Data
```{r}

train = read.csv("train.csv")
test = read.csv("test.csv")

```

## Prepare Data for XGBoost
```{r}

# Point Diff

# Create training matrix
pd_train_matrix = xgb.DMatrix(data = as.matrix(train[,1:596]), 
                              label = as.numeric(train$t1_point_diff))

# Create training matrix
pd_test_matrix = xgb.DMatrix(data = as.matrix(test[,1:596]), 
                              label = as.numeric(test$t1_point_diff))




# Total Points

# Create training matrix
tp_train_matrix = xgb.DMatrix(data = as.matrix(train[,1:595]), 
                              label = as.numeric(train$total_points))

# Create training matrix
tp_test_matrix = xgb.DMatrix(data = as.matrix(test[,1:595]), 
                              label = as.numeric(test$total_points))




# Win Prob

# Create training matrix
wp_train_matrix = xgb.DMatrix(data = as.matrix(train[,1:595]), 
                              label = as.numeric(train$t1_win))

# Create training matrix
wp_test_matrix = xgb.DMatrix(data = as.matrix(test[,1:595]), 
                              label = as.numeric(test$t1_win))

```



Next, let's tune ETA (learning rate).

```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_1 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.3, # Set learning rate
                     
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_2 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_3 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_4 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .01,
               
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_5 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```


```{r}

# eta plots

# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7

```











### Tune 2






```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_12 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.04, # Set learning rate
                     
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_22 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.03, # Set learning rate

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_32 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.02, # Set learning rate
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_42 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .009,
               
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_52 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.008, # Set learning rate
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```



```{r}

# eta plots

# Extract results for model with eta = 0.3
pd12 <- cbind.data.frame(bst_mod_12$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.04, nrow(bst_mod_12$evaluation_log)))
names(pd12)[3] <- "eta"
# Extract results for model with eta = 0.1
pd22 <- cbind.data.frame(bst_mod_22$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.03, nrow(bst_mod_22$evaluation_log)))
names(pd22)[3] <- "eta"
# Extract results for model with eta = 0.05
pd32 <- cbind.data.frame(bst_mod_32$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.02, nrow(bst_mod_32$evaluation_log)))
names(pd32)[3] <- "eta"
# Extract results for model with eta = 0.01
pd42 <- cbind.data.frame(bst_mod_42$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.009, nrow(bst_mod_42$evaluation_log)))
names(pd42)[3] <- "eta"
# Extract results for model with eta = 0.005
pd52 <- cbind.data.frame(bst_mod_52$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.008, nrow(bst_mod_52$evaluation_log)))
names(pd52)[3] <- "eta"
# Join datasets
plot_data2 <- rbind.data.frame(pd12, pd22, pd32, pd42, pd52)
# Converty ETA to factor
plot_data2$eta <- as.factor(plot_data2$eta)
# Plot points
g_62 <- ggplot(plot_data2, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_62

# Plot lines
g_72 <- ggplot(plot_data2, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_72

```





Max Depth and Min Child Weight Next:

```{r}
# Be Careful - This can take a very long time to run
max_depth_vals <- c(3, 5, 7, 10) # Create vector of max depth values
min_child_weight <- c(3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.04, # Set learning rate
                     
                    
                     max.depth = cv_params$max_depth[i], # Set max depth
                     min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print AUC heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "error") # Set labels
g_2 # Generate plot



```




Gamma Tuning

```{r, results='hide', echo=TRUE}
###### 2 - Gamma Tuning ######


gamma_vals <- c(0, 0.05, 0.1, 0.15, 0.2, 0.5) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(844)
error_vec  <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.04, # Set learning rate
                     
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = gamma_vals[i], # Set minimum loss reduction for split
                     
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Prints out result every 20th iteration,
                     
                     eval_metric = "rmse",
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
}
  
```

```{r}

# Lets view our results to identify the value of gamma to use:

# Gamma results
# Join gamma to values
cbind.data.frame(gamma_vals, error_vec)

```




```{r, results='hide', echo=TRUE}
###### 2 - Gamma Tuning ######


gamma_vals <- c(.08,.09, .11, .12, .6) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(844)
error_vec  <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.04, # Set learning rate
                     
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = gamma_vals[i], # Set minimum loss reduction for split
                     
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Prints out result every 20th iteration,
                     
                     eval_metric = "rmse",
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
}
  
```

```{r}

# Lets view our results to identify the value of gamma to use:

# Gamma results
# Join gamma to values
cbind.data.frame(gamma_vals, error_vec)

```








Alpha and Lamdba:

```{r}
# Be Careful - This can take a very long time to run
alpha_vals <- c(0, .1, .5) # Create vector of max depth values
lambda_vals <- c(.1, 1, 3) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(alpha_vals, lambda_vals)
names(cv_params) <- c("alpha", "lambda")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.04, # Set learning rate
                     
                    
                     alpha = cv_params$alpha[i], # Set max depth
                     lambda = cv_params$lambda[i], # Set minimum number of samples in node to split
                     gamma = .10,
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$alpha <- as.factor(res_db$alpha) # Convert tree number to factor for plotting
res_db$lambda <- as.factor(res_db$lambda) # Convert node size to factor for plotting
# Print AUC heatmap
g_3 <- ggplot(res_db, aes(y = alpha, x = lambda, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Lambda", y = "Alpha", fill = "error") # Set labels
g_3 # Generate plot



```


```{r}
# Be Careful - This can take a very long time to run
alpha_vals <- c(0, .1, .5) # Create vector of max depth values
lambda_vals <- c(3, 3.5, 4) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(alpha_vals, lambda_vals)
names(cv_params) <- c("alpha", "lambda")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.04, # Set learning rate
                     
                    
                     alpha = cv_params$alpha[i], # Set max depth
                     lambda = cv_params$lambda[i], # Set minimum number of samples in node to split
                     gamma = .10,
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$alpha <- as.factor(res_db$alpha) # Convert tree number to factor for plotting
res_db$lambda <- as.factor(res_db$lambda) # Convert node size to factor for plotting
# Print AUC heatmap
g_3 <- ggplot(res_db, aes(y = alpha, x = lambda, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Lambda", y = "Alpha", fill = "error") # Set labels
g_3 # Generate plot



```



```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_1 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
                     
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_2 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_3 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.04, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_4 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .01,
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
               
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_5 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```


```{r}

# eta plots

# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.04, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7

```























































Now, let's tune our model. Let's start with iterations. We'll start with slow learning rate for complex problem

```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
               eta = 0.005, # Set learning rate
              
               nrounds = 20000, # Set number of rounds
               early_stopping_rounds = 800, # Set number of rounds to stop at if there is no improvement
               
               verbose = 1, # 1 - Prints out fit
               nthread = 1, # Set number of parallel threads
               print_every_n = 20, # Prints out result every 20th iteration
              
               
               eval_metric = "rmse",
               ) # Set evaluation metric to use
```


nrounds = 1434, let's keep our nrounds kinda high


Max Depth and Min Child Weight Next:

```{r}
# Be Careful - This can take a very long time to run
max_depth_vals <- c(3, 5, 7, 10, 15) # Create vector of max depth values
min_child_weight <- c(1,3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.005, # Set learning rate
                     
                    
                     max.depth = cv_params$max_depth[i], # Set max depth
                     min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
                     
                     
                     nrounds = 3000, # Set number of rounds
                     early_stopping_rounds = 1000, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print AUC heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "error") # Set labels
g_2 # Generate plot



```

Definitely 3 Depth, Will try different Child Weights but like 5, 7, or 10

Gamma Tuning

```{r, results='hide', echo=TRUE}
###### 2 - Gamma Tuning ######


gamma_vals <- c(0, 0.05, 0.1, 0.15, 0.2) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(844)
error_vec  <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.005, # Set learning rate
                     
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = gamma_vals[i], # Set minimum loss reduction for split
                     
                     
                     
                     nrounds = 3000, # Set number of rounds
                     early_stopping_rounds = 1000, # Prints out result every 20th iteration,
                     
                     eval_metric = "rmse",
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
}
  
```

```{r}

# Lets view our results to identify the value of gamma to use:

# Gamma results
# Join gamma to values
cbind.data.frame(gamma_vals, error_vec)

```

.05 and .15 close, going with .05

Re-calibrate optimal rounds

```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
                     
              max.depth = 7, # Set max depth
              min_child_weight = 3, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
             
               
              nrounds = 3000, # Set number of rounds
              early_stopping_rounds = 1000, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```

Let's increase rounda a lot for our tuning, numbers getting high




```{r}

# eta plots

# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7

```

ETA = .005

Finally, let's re-tune our optimal iterations.

```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              
              eta = 0.005, # Set learning rate
                     
              max.depth = 3, # Set max depth
              min_child_weight = 5, # Set minimum number of samples in node to split
              gamma = .05, # Set minimum loss reduction for split
                     #.15
                     
                     
              nrounds = 100000, # Set number of rounds
              early_stopping_rounds = 1000, # Prints out result every 20th iteration,
                     
            
              
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```

nrounds = 3507

At last, let's try our optimal model (tried some different Child Weights and .05/.15 Gamma, 5 and .05 gave best results).

```{r}
set.seed(844)
bst_final <- xgboost(data = pd_train_matrix, # Set training data
              
        
               
              eta = 0.005, # Set learning rate
                     
              max.depth = 3, # Set max depth
              min_child_weight = 5, # Set minimum number of samples in node to split
              gamma = .05, # Set minimum loss reduction for split
               
              nrounds = 3507 , # Set number of rounds
              early_stopping_rounds = 1000, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use



```

```{r}

boost_preds_final <- predict(bst_final, pd_test_matrix) # Create predictions for xgboost model

pred_dat_final <- cbind.data.frame(boost_preds_final, test$t1_point_diff)

rmse(test$t1_point_diff, boost_preds_final)


pred_dat_final

```

```{r}

pred_dat_win_final = pred_dat_final %>% 
  mutate(actual_win = ifelse(`test$t1_point_diff` > 0, 1, 0)) %>% 
  mutate(pred_win = ifelse(boost_preds_final > 0, 1, 0))


# Create confusion matrix
conf_matrix_final <- confusionMatrix(as.factor(pred_dat_win_final$pred_win),
                                     as.factor(pred_dat_win_final$actual_win), positive = "1")

# Print confusion matrix and sensitivity
print(conf_matrix_final)

```


```{r}
plot_dat <- cbind.data.frame(test$t1_point_diff,boost_preds_final )
names(plot_dat) <- c("actual", "predicted")

ggplot(plot_dat, aes(x = actual, y = predicted)) +
  geom_point() +
  geom_smooth()
```

```{r}
# Extract importance
imp_mat <- xgb.importance(model = bst_final)
# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 10)
```






