---
title: "CFB Game Predictive Model"
author: "The Five Horsemen"
date: "2024-10-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Install Packages
```{r, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(rsample)
library(mice)
library(fastDummies)
library(xgboost)
library(Metrics)
library(caret)
```

## Load Data
```{r}

train = read.csv("train.csv")
test = read.csv("test.csv")

pd_vars = scan("pd_vars.txt", what = "character", sep = "\n")
move_cols = scan("move_cols.txt", what = "character", sep = "\n")

```

## Prepare Data for XGBoost
```{r}

# Point Diff

pd_train = train %>% select(c(pd_vars, move_cols))
pd_test = test %>% select(c(pd_vars, move_cols))


# Create training matrix
pd_train_matrix = xgb.DMatrix(data = as.matrix(pd_train[,1:223]), 
                              label = as.numeric(train$t1_point_diff))

# Create training matrix
pd_test_matrix = xgb.DMatrix(data = as.matrix(pd_test[,1:223]), 
                              label = as.numeric(test$t1_point_diff))




# Total Points

# Create training matrix
tp_train_matrix = xgb.DMatrix(data = as.matrix(train[,1:595]), 
                              label = as.numeric(train$total_points))

# Create training matrix
tp_test_matrix = xgb.DMatrix(data = as.matrix(test[,1:595]), 
                              label = as.numeric(test$total_points))




# Win Prob

# Create training matrix
wp_train_matrix = xgb.DMatrix(data = as.matrix(train[,1:595]), 
                              label = as.numeric(train$t1_win))

# Create training matrix
wp_test_matrix = xgb.DMatrix(data = as.matrix(test[,1:595]), 
                              label = as.numeric(test$t1_win))

```



Next, let's tune ETA (learning rate).

```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_1 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.3, # Set learning rate
                     
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_2 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_3 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_4 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .01,
               
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_5 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```


```{r}

# eta plots

# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7

```











### Tune 2






```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_12 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.04, # Set learning rate
                     
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_22 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.03, # Set learning rate

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_32 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.02, # Set learning rate
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_42 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .009,
               
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_52 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.008, # Set learning rate
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 100, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```



```{r}

# eta plots

# Extract results for model with eta = 0.3
pd12 <- cbind.data.frame(bst_mod_12$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.04, nrow(bst_mod_12$evaluation_log)))
names(pd12)[3] <- "eta"
# Extract results for model with eta = 0.1
pd22 <- cbind.data.frame(bst_mod_22$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.03, nrow(bst_mod_22$evaluation_log)))
names(pd22)[3] <- "eta"
# Extract results for model with eta = 0.05
pd32 <- cbind.data.frame(bst_mod_32$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.02, nrow(bst_mod_32$evaluation_log)))
names(pd32)[3] <- "eta"
# Extract results for model with eta = 0.01
pd42 <- cbind.data.frame(bst_mod_42$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.009, nrow(bst_mod_42$evaluation_log)))
names(pd42)[3] <- "eta"
# Extract results for model with eta = 0.005
pd52 <- cbind.data.frame(bst_mod_52$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.008, nrow(bst_mod_52$evaluation_log)))
names(pd52)[3] <- "eta"
# Join datasets
plot_data2 <- rbind.data.frame(pd12, pd22, pd32, pd42, pd52)
# Converty ETA to factor
plot_data2$eta <- as.factor(plot_data2$eta)
# Plot points
g_62 <- ggplot(plot_data2, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_62

# Plot lines
g_72 <- ggplot(plot_data2, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_72

```





Max Depth and Min Child Weight Next:

```{r}
# Be Careful - This can take a very long time to run
max_depth_vals <- c(3, 5, 7, 10) # Create vector of max depth values
min_child_weight <- c(3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.04, # Set learning rate
                     
                    
                     max.depth = cv_params$max_depth[i], # Set max depth
                     min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print AUC heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "error") # Set labels
g_2 # Generate plot



```




Gamma Tuning

```{r, results='hide', echo=TRUE}
###### 2 - Gamma Tuning ######


gamma_vals <- c(0, 0.05, 0.1, 0.15, 0.2, 0.5) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(844)
error_vec  <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.04, # Set learning rate
                     
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = gamma_vals[i], # Set minimum loss reduction for split
                     
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Prints out result every 20th iteration,
                     
                     eval_metric = "rmse",
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
}
  
```

```{r}

# Lets view our results to identify the value of gamma to use:

# Gamma results
# Join gamma to values
cbind.data.frame(gamma_vals, error_vec)

```




```{r, results='hide', echo=TRUE}
###### 2 - Gamma Tuning ######


gamma_vals <- c(.08,.09, .11, .12, .6) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(844)
error_vec  <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.04, # Set learning rate
                     
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = gamma_vals[i], # Set minimum loss reduction for split
                     
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Prints out result every 20th iteration,
                     
                     eval_metric = "rmse",
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
}
  
```

```{r}

# Lets view our results to identify the value of gamma to use:

# Gamma results
# Join gamma to values
cbind.data.frame(gamma_vals, error_vec)

```








Alpha and Lamdba:

```{r}
# Be Careful - This can take a very long time to run
alpha_vals <- c(0, .1, .5) # Create vector of max depth values
lambda_vals <- c(.1, 1, 3) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(alpha_vals, lambda_vals)
names(cv_params) <- c("alpha", "lambda")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.04, # Set learning rate
                     
                    
                     alpha = cv_params$alpha[i], # Set max depth
                     lambda = cv_params$lambda[i], # Set minimum number of samples in node to split
                     gamma = .10,
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$alpha <- as.factor(res_db$alpha) # Convert tree number to factor for plotting
res_db$lambda <- as.factor(res_db$lambda) # Convert node size to factor for plotting
# Print AUC heatmap
g_3 <- ggplot(res_db, aes(y = alpha, x = lambda, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Lambda", y = "Alpha", fill = "error") # Set labels
g_3 # Generate plot



```


```{r}
# Be Careful - This can take a very long time to run
alpha_vals <- c(0, .1, .5) # Create vector of max depth values
lambda_vals <- c(3, 3.5, 4) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(alpha_vals, lambda_vals)
names(cv_params) <- c("alpha", "lambda")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.04, # Set learning rate
                     
                    
                     alpha = cv_params$alpha[i], # Set max depth
                     lambda = cv_params$lambda[i], # Set minimum number of samples in node to split
                     gamma = .10,
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     
                     
                     
                     nrounds = 450, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$alpha <- as.factor(res_db$alpha) # Convert tree number to factor for plotting
res_db$lambda <- as.factor(res_db$lambda) # Convert node size to factor for plotting
# Print AUC heatmap
g_3 <- ggplot(res_db, aes(y = alpha, x = lambda, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Lambda", y = "Alpha", fill = "error") # Set labels
g_3 # Generate plot



```



```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_1 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
                     
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_2 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_3 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.04, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_4 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .01,
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
               
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_5 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```


```{r}

# eta plots

# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.04, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7

```




***** with new variables now *******





```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_1 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
                     
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_2 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_3 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.04, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_4 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .01,
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
               
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_5 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```


```{r}

# eta plots

# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.04, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7

```




```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_12 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.08, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
                     
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_22 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.07, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_32 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.06, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_42 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .03,
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_52 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.02, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .1, # Set minimum loss reduction for split,
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```

```{r}

# eta plots

# Extract results for model with eta = 0.3
pd12 <- cbind.data.frame(bst_mod_12$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.08, nrow(bst_mod_12$evaluation_log)))
names(pd12)[3] <- "eta"
# Extract results for model with eta = 0.1
pd22 <- cbind.data.frame(bst_mod_22$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.07, nrow(bst_mod_22$evaluation_log)))
names(pd22)[3] <- "eta"
# Extract results for model with eta = 0.05
pd32 <- cbind.data.frame(bst_mod_32$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.06, nrow(bst_mod_32$evaluation_log)))
names(pd32)[3] <- "eta"
# Extract results for model with eta = 0.01
pd42 <- cbind.data.frame(bst_mod_42$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.03, nrow(bst_mod_42$evaluation_log)))
names(pd42)[3] <- "eta"
# Extract results for model with eta = 0.005
pd52 <- cbind.data.frame(bst_mod_52$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.02, nrow(bst_mod_52$evaluation_log)))
names(pd52)[3] <- "eta"
# Join datasets
plot_data2 <- rbind.data.frame(pd2, pd3)
# Converty ETA to factor
plot_data2$eta <- as.factor(plot_data2$eta)
# Plot points
g_62 <- ggplot(plot_data2, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_62

# Plot lines
g_72 <- ggplot(plot_data2, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_72



```





Gamma Tuning

```{r, results='hide', echo=TRUE}
###### 2 - Gamma Tuning ######


gamma_vals <- c(0, 0.05, 0.1, 0.15, 0.2, 0.5) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(844)
error_vec  <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.05, # Set learning rate
                     
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = gamma_vals[i], # Set minimum loss reduction for split
                     
                     
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Prints out result every 20th iteration,
                     
                     eval_metric = "rmse",
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
}
  
```

```{r}

# Lets view our results to identify the value of gamma to use:

# Gamma results
# Join gamma to values
cbind.data.frame(gamma_vals, error_vec)

```


```{r, results='hide', echo=TRUE}
###### 2 - Gamma Tuning ######


gamma_vals <- c(.03, 0.07, 0.7) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(844)
error_vec  <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.05, # Set learning rate
                     
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = gamma_vals[i], # Set minimum loss reduction for split
                     
                     
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Prints out result every 20th iteration,
                     
                     eval_metric = "rmse",
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
}
  
```

```{r}

# Lets view our results to identify the value of gamma to use:

# Gamma results
# Join gamma to values
cbind.data.frame(gamma_vals, error_vec)

```









```{r}
###### 3 - Subsample and Column sample Tuning ######

# Be Careful - This can take a very long time to run
subsample <- c(0.4, 0.7, 0.9, 1) # Create vector of subsample values
colsample_by_tree <- c(0.4, 0.7, 0.9, 1) # Create vector of col sample values

# Expand grid of tuning parameters
cv_params <- expand.grid(subsample, colsample_by_tree)
names(cv_params) <- c("subsample", "colsample_by_tree")
# Create vectors to store results
rmse_vec <- rep(NA, nrow(cv_params)) 
# Loop through parameter values
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.05, # Set learning rate
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = .05, # Set minimum loss reduction for split
                     subsample = cv_params$subsample[i], # Set proportion of training data to use in tree
                     colsample_bytree = cv_params$colsample_by_tree[i], # Set number of variables to use in each tree
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

```




```{r}

# visualise tuning sample params

res_db <- cbind.data.frame(cv_params, rmse_vec)
names(res_db)[3] <- c("rmse") 
res_db$subsample <- as.factor(res_db$subsample) # Convert tree number to factor for plotting
res_db$colsample_by_tree <- as.factor(res_db$colsample_by_tree) # Convert node size to factor for plotting
g_4 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$rmse), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "RMSE") # Set labels
g_4 # Generate plot

```



```{r}
###### 3 - Subsample and Column sample Tuning ######

# Be Careful - This can take a very long time to run
subsample <- c(0.5, 0.6, 0.8, 1) # Create vector of subsample values
colsample_by_tree <- c(0.9, 1) # Create vector of col sample values

# Expand grid of tuning parameters
cv_params <- expand.grid(subsample, colsample_by_tree)
names(cv_params) <- c("subsample", "colsample_by_tree")
# Create vectors to store results
rmse_vec <- rep(NA, nrow(cv_params)) 
# Loop through parameter values
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.05, # Set learning rate
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = .05, # Set minimum loss reduction for split
                     subsample = cv_params$subsample[i], # Set proportion of training data to use in tree
                     colsample_bytree = cv_params$colsample_by_tree[i], # Set number of variables to use in each tree
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

```




```{r}

# visualise tuning sample params

res_db <- cbind.data.frame(cv_params, rmse_vec)
names(res_db)[3] <- c("rmse") 
res_db$subsample <- as.factor(res_db$subsample) # Convert tree number to factor for plotting
res_db$colsample_by_tree <- as.factor(res_db$colsample_by_tree) # Convert node size to factor for plotting
g_4 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$rmse), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "RMSE") # Set labels
g_4 # Generate plot

```


```{r}
###### 3 - Subsample and Column sample Tuning ######

# Be Careful - This can take a very long time to run
subsample <- c(.75, .85, .9) # Create vector of subsample values
colsample_by_tree <- c(1) # Create vector of col sample values

# Expand grid of tuning parameters
cv_params <- expand.grid(subsample, colsample_by_tree)
names(cv_params) <- c("subsample", "colsample_by_tree")
# Create vectors to store results
rmse_vec <- rep(NA, nrow(cv_params)) 
# Loop through parameter values
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     eta = 0.05, # Set learning rate
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = .05, # Set minimum loss reduction for split
                     subsample = cv_params$subsample[i], # Set proportion of training data to use in tree
                     colsample_bytree = cv_params$colsample_by_tree[i], # Set number of variables to use in each tree
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints out fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20 # Prints out result every 20th iteration
  ) # Set evaluation metric to use
  
  
  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

```

```{r}

# visualise tuning sample params

res_db <- cbind.data.frame(cv_params, rmse_vec)
names(res_db)[3] <- c("rmse") 
res_db$subsample <- as.factor(res_db$subsample) # Convert tree number to factor for plotting
res_db$colsample_by_tree <- as.factor(res_db$colsample_by_tree) # Convert node size to factor for plotting
g_4 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$rmse), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "RMSE") # Set labels
g_4 # Generate plot

```



```{r}
# Be Careful - This can take a very long time to run
max_depth_vals <- c(2, 3, 5, 7) # Create vector of max depth values
min_child_weight <- c(1,3,5,7, 10,15) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.05, # Set learning rate
                     gamma = .05, # Set minimum loss reduction for split
                     subsample = .8, # Set proportion of training data to use in tree
                     colsample_bytree = 1,
                     
                    
                     max.depth = cv_params$max_depth[i], # Set max depth
                     min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
                     
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print AUC heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "error") # Set labels
g_2 # Generate plot



```



```{r}
# Be Careful - This can take a very long time to run
max_depth_vals <- c(3) # Create vector of max depth values
min_child_weight <- c(13, 17, 20) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.05, # Set learning rate
                     gamma = .05, # Set minimum loss reduction for split
                     subsample = .8, # Set proportion of training data to use in tree
                     colsample_bytree = 1,
                     
                    
                     max.depth = cv_params$max_depth[i], # Set max depth
                     min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
                     
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print AUC heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "error") # Set labels
g_2 # Generate plot



```




```{r}
# Be Careful - This can take a very long time to run
alpha_vals <- c(0, .1, .5) # Create vector of max depth values
lambda_vals <- c(1, 3, 3.5, 4) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(alpha_vals, lambda_vals)
names(cv_params) <- c("alpha", "lambda")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.05, # Set learning rate
                     
                    
                     alpha = cv_params$alpha[i], # Set max depth
                     lambda = cv_params$lambda[i], # Set minimum number of samples in node to split
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = .05, # Set minimum loss reduction for split
                     subsample = .8, # Set proportion of training data to use in tree
                     colsample_bytree = 1,
                     
                     
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$alpha <- as.factor(res_db$alpha) # Convert tree number to factor for plotting
res_db$lambda <- as.factor(res_db$lambda) # Convert node size to factor for plotting
# Print AUC heatmap
g_3 <- ggplot(res_db, aes(y = alpha, x = lambda, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Lambda", y = "Alpha", fill = "error") # Set labels
g_3 # Generate plot



```


```{r}
# Be Careful - This can take a very long time to run
alpha_vals <- c(0) # Create vector of max depth values
lambda_vals <- c(.8, 1, 1.5, 2) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(alpha_vals, lambda_vals)
names(cv_params) <- c("alpha", "lambda")
# Create results vector
error_vec  <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(844)
  bst_tune <- xgb.cv(data = pd_train_matrix, # Set training data
                     
                     nfold = 5, # Use 5 fold cross-validation
                     
                     
                     eta = 0.05, # Set learning rate
                     
                    
                     alpha = cv_params$alpha[i], # Set max depth
                     lambda = cv_params$lambda[i], # Set minimum number of samples in node to split
                     max.depth = 3, # Set max depth
                     min_child_weight = 7, # Set minimum number of samples in node to split
                     gamma = .05, # Set minimum loss reduction for split
                     subsample = .8, # Set proportion of training data to use in tree
                     colsample_bytree = 1,
                     
                     
                     
                     nrounds = 140, # Set number of rounds
                     early_stopping_rounds = 10, # Set number of rounds to stop at if there is no improvement
                     
                     verbose = 1, # 1 - Prints oSut fit
                     nthread = 1, # Set number of parallel threads
                     print_every_n = 20, # Prints out result every 20th iteration,
                     
                     
                     eval_metric = "rmse" 
                     
  ) # Set evaluation metric to use
  
  error_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  
  
}

bst_tune$evaluation_log


# Join results in dataset
res_db <- cbind.data.frame(cv_params, error_vec)
names(res_db)[3] <- c("error") 
res_db$alpha <- as.factor(res_db$alpha) # Convert tree number to factor for plotting
res_db$lambda <- as.factor(res_db$lambda) # Convert node size to factor for plotting
# Print AUC heatmap
g_3 <- ggplot(res_db, aes(y = alpha, x = lambda, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$error), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Lambda", y = "Alpha", fill = "error") # Set labels
g_3 # Generate plot



```





```{r eta tuning}

# haven't tuned the other stuff yet, comment out for now

#Let's try out different ETAs

# Use xgb.cv to run cross-validation inside xgboost
set.seed(844)
bst_mod_12 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = .06,
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .05, # Set minimum loss reduction for split
              subsample = .8, # Set proportion of training data to use in tree
              colsample_bytree = 1,
                     
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use


set.seed(844)
bst_mod_22 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .05,
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .05, # Set minimum loss reduction for split
              subsample = .8, # Set proportion of training data to use in tree
              colsample_bytree = 1,

               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_32 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = .04,
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .05, # Set minimum loss reduction for split
              subsample = .8, # Set proportion of training data to use in tree
              colsample_bytree = 1,
             
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use
set.seed(844)
bst_mod_42 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
              
              eta = .03,
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .05, # Set minimum loss reduction for split
              subsample = .8, # Set proportion of training data to use in tree
              colsample_bytree = 1,
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
                
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              eval_metric = "rmse") # Set evaluation metric to use

set.seed(844)
bst_mod_52 <- xgb.cv(data = pd_train_matrix, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.02, # Set learning rate
              max.depth = 3, # Set max depth
              min_child_weight = 7, # Set minimum number of samples in node to split
              gamma = .05, # Set minimum loss reduction for split
              subsample = .8, # Set proportion of training data to use in tree
              colsample_bytree = 1,
              
               
              nrounds = 10000, # Set number of rounds
              early_stopping_rounds = 30, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              eval_metric = "rmse") # Set evaluation metric to use
```

```{r}

# eta plots

# Extract results for model with eta = 0.3
pd12 <- cbind.data.frame(bst_mod_12$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.06, nrow(bst_mod_12$evaluation_log)))
names(pd12)[3] <- "eta"
# Extract results for model with eta = 0.1
pd22 <- cbind.data.frame(bst_mod_22$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(bst_mod_22$evaluation_log)))
names(pd22)[3] <- "eta"
# Extract results for model with eta = 0.05
pd32 <- cbind.data.frame(bst_mod_32$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.04, nrow(bst_mod_32$evaluation_log)))
names(pd32)[3] <- "eta"
# Extract results for model with eta = 0.01
pd42 <- cbind.data.frame(bst_mod_42$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.03, nrow(bst_mod_42$evaluation_log)))
names(pd42)[3] <- "eta"
# Extract results for model with eta = 0.005
pd52 <- cbind.data.frame(bst_mod_52$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.02, nrow(bst_mod_52$evaluation_log)))
names(pd52)[3] <- "eta"
# Join datasets
plot_data2 <- rbind.data.frame(pd22, pd32)
# Converty ETA to factor
plot_data2$eta <- as.factor(plot_data2$eta)
# Plot points
g_62 <- ggplot(plot_data2, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_62

# Plot lines
g_72 <- ggplot(plot_data2, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_72



```


##### stopped here tuning, notes below ###




































